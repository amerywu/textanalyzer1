[extract_instructions]
provider = corpus_rake

# options: job
# possible-phrase
# corpus_text_ranK
#corpus_lda
# reddit
#

query_type = fetch_all
# options
#fetch_all
#field_query

query_field =analysis_type
#analysis_type
query_value = Gensim_LDA_by_subset_NounPhraseResultsToLinkedDocList
#Gensim_LDA_by_subset_NounPhraseResultsToLinkedDocList

all_providers= job
ignore_indices=xdx

[conn_elasticsearch]
#url = host.docker.internal
#url = localhost
#port = 9200
#scheme = http
#auth = false


url =192.168.0.141
scheme=http
port=9200
auth = true

[pipeline_instructions]
testenv = false
testenv_doc_process_count = 1000
group_testenv = false
group_testenv_list = COMPUTER SCIENCE
#HUMAN RESOURCE MANAGEMENT,EDUCATION,MARKETING,COMPUTER SCIENCE

pipeline_name = _category_prediction
# options:
#_category_prediction
#_text_rank
#_job_integrity_analysis
#gensim_lda,
#sklearn_lda,
#gensim_lda_by_subset,
#lda_topic_comparator
#save_as_csv : Pipeline for saving ES data as a CSV.
#_rake
#_group_by_column
#_text_rank

post_process = none
# options:
#post process for job analysis
#text_rank_output_by_group,rake_output_by_group,noun_phrase_output_by_group,gensim_lda_report_by_subset

#text_rank_output_by_group
#rake_output_by_group
#tfidf_partof_sentence_breakout, This is the post process to update ES after completing sirf_tfidf pipeline
#rake
#page_views_confluence,
#gensim_lda_report,
#gensim_lda_report_by_subset,
#tfidf_log_text_detector,
#gensim_lda_report_topic_similarity,
#save_dictionaries_to_file
#major_analysis
#none

continue_run = true
run_forever = false

[ml_instructions]
#######
# for all gensim lda
######
gensim_lda_topics = 5
gensim_lda_term_per_topic_reporting_count = 12
gensim_lda_report_sentence_level = true

#######
# for  gensim lda only when analyzing data subsets
######
gensim_lda_permitted_term_overlap_across_topics = 2
gensim_lda_topic_similarity_threshold = 3
gensim_top_level_topic_csv = ./resources/flatfiles/lda_topics_toplevel.csv

#######
# for stop word generation
#########
stopword_top_threshold = 0.003
stopword_bottom_threshold = 0.000005
#######
# for text_cleaning
#########
text_fields_to_clean=skills
strip_tags = True
strip_short = False
strip_nonalphanumeric = True
strip_punctuation = True
strip_multispaces = True
strip_stop_phrases = True
split_alphanum = True
convert_to_lower = True

put_cleaned_text_in_analysis_dict = False

#######
# for sub_groups
######
minimum_doc_count = 250

##############
# for scikit corpus preparation
##################

vectorizer_type = tfidf

############
# for scikit RF
###########
rf_test_proportion = 0.1
rf_category = group_by
rf_max_features = 4000
#######
# for rake
#########
rake_textfield_in_df = skills

######
# for text_rank
#######

text_rank_top_n_rankings = 5

#######
# for df groupby
#########
df_groupby_column=possible-phrase

[local_data]
lda_topics_toplevel_raw  = ./resources/flatfiles/lda_topics_toplevel_raw.csv
lda_topics_by_subset_raw = ./resources/flatfiles/lda_topics_by_subset_raw.csv
aggregated_majors_filepath = ./resources/aggregatedMajors.json
job_to_majors_filepath = ./resources/jobToMajor.json

[job_instructions]
es_file_location = D:\jake\filedump\data_extracted\output_data.csv
output_folder = D:\jake\filedump\data_extracted\reports
stop_list= ./resources/nlp_inputs/SmartStoplist.txt
stop_phrases= ./resources/nlp_inputs/stop_phrases
stop_list_folder= ./resources/nlp_inputs/
output_to_elasticsearch = true



filter_space_include = NounPhrase
#lemmatized
filter_space_exclude =
#RELATED,ANY,RELEVANT,None
filter_group_include = lemmatized
filter_group_exclude = RELATED,ANY,RELEVANT,None